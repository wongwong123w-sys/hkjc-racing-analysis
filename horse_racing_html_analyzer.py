# -*- coding: utf-8 -*-

"""
å¾€ç¸¾è¡¨æ ¼ HTML çµæ§‹è¨ºæ–·å·¥å…·
Racing History HTML Structure Analyzer

ç”¨é€”: åˆ†æå¯¦éš›çš„ HKJC ç¶²é çµæ§‹ï¼Œæ‰¾å‡ºå¾€ç¸¾è¡¨æ ¼ä½ç½®å’Œæ¬„ä½
"""

import requests
from bs4 import BeautifulSoup
import json

def diagnose_horse_page(horse_id: str = "HK_2023_J411"):
    """
    å®Œæ•´è¨ºæ–·é¦¬åŒ¹è³‡æ–™é çš„ HTML çµæ§‹
    """
    
    url = f"https://racing.hkjc.com/zh-hk/local/information/horse?horseid={horse_id}"
    
    print(f"ğŸ” è¨ºæ–· {horse_id}")
    print(f"URL: {url}\n")
    
    # çˆ¬å–ç¶²é 
    response = requests.get(url, timeout=15)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # ============================================================
    # 1. åˆ—å‡ºæ‰€æœ‰è¡¨æ ¼
    # ============================================================
    print("=" * 70)
    print("1ï¸âƒ£ æ‰€æœ‰è¡¨æ ¼çµ±è¨ˆ")
    print("=" * 70)
    
    tables = soup.find_all('table')
    print(f"âœ“ æ‰¾åˆ° {len(tables)} å€‹è¡¨æ ¼\n")
    
    for idx, table in enumerate(tables):
        rows = table.find_all('tr')
        cols = len(table.find_all('th')) + len(table.find_all('td'))
        table_class = table.get('class', [])
        table_id = table.get('id', '')
        
        print(f"è¡¨ {idx}:")
        print(f"  class: {table_class}")
        print(f"  id: {table_id}")
        print(f"  è¡Œæ•¸: {len(rows)}")
        print(f"  ä¼°è¨ˆæ¬„æ•¸: {cols}")
        
        # æå–è¡¨é ­
        headers = table.find_all('th')
        if headers:
            header_texts = [h.get_text(strip=True)[:15] for h in headers[:10]]
            print(f"  è¡¨é ­ (å‰ 10 æ¬„): {header_texts}")
        
        print()
    
    # ============================================================
    # 2. è©³ç´°åˆ†ææ¯å€‹è¡¨æ ¼
    # ============================================================
    print("=" * 70)
    print("2ï¸âƒ£ è©³ç´°è¡¨æ ¼åˆ†æ")
    print("=" * 70 + "\n")
    
    for idx, table in enumerate(tables):
        print(f"ğŸ“Š è¡¨ {idx} è©³ç´°ä¿¡æ¯:")
        print("-" * 70)
        
        # è¡¨æ ¼å±¬æ€§
        print(f"å±¬æ€§:")
        print(f"  class: {table.get('class', [])}")
        print(f"  id: {table.get('id', '')}")
        print(f"  style: {table.get('style', '')[:80]}")
        
        # è¡¨é ­
        headers = table.find_all('th')
        print(f"\nè¡¨é ­ ({len(headers)} æ¬„):")
        for h_idx, h in enumerate(headers[:20]):
            print(f"  {h_idx}: {h.get_text(strip=True)}")
        
        # ç¬¬ä¸€è¡Œæ•¸æ“š
        rows = table.find_all('tr')
        if len(rows) > 1:
            first_row = rows[1]
            cells = first_row.find_all('td')
            print(f"\nç¬¬ä¸€è¡Œæ•¸æ“š ({len(cells)} æ¬„):")
            for c_idx, cell in enumerate(cells[:10]):
                cell_text = cell.get_text(strip=True)[:30]
                print(f"  {c_idx}: {cell_text}")
        
        # åˆ¤æ–·æ˜¯å¦å¯èƒ½æ˜¯å¾€ç¸¾è¡¨
        header_combined = ' '.join([h.get_text(strip=True) for h in headers]).lower()
        keywords = ['å ´æ¬¡', 'åæ¬¡', 'æ—¥æœŸ', 'é¦¬å ´', 'è·é›¢', 'è©•åˆ†', 'å¾€ç¸¾', 'racing', 'history']
        keyword_matches = [k for k in keywords if k.lower() in header_combined]
        
        print(f"\nå¯èƒ½æ€§:")
        print(f"  è¡Œæ•¸: {len(rows)} {'âœ“ (æœ‰æ•¸æ“š)' if len(rows) > 1 else 'âœ— (ç„¡æ•¸æ“š)'}")
        print(f"  æ¬„æ•¸: {len(headers)} {'âœ“ (å¯èƒ½æ˜¯å¾€ç¸¾)' if 10 <= len(headers) <= 20 else 'âœ—'}")
        print(f"  é—œéµè©åŒ¹é…: {keyword_matches} {'âœ“' if len(keyword_matches) >= 2 else 'âœ—'}")
        
        print()
    
    # ============================================================
    # 3. æŸ¥æ‰¾é—œéµå…ƒç´ 
    # ============================================================
    print("=" * 70)
    print("3ï¸âƒ£ é é¢çµæ§‹åˆ†æ")
    print("=" * 70 + "\n")
    
    # æŸ¥æ‰¾åŒ…å«ã€Œå¾€ç¸¾ã€çš„æ–‡æœ¬
    all_text = soup.get_text()
    if 'å¾€ç¸¾' in all_text:
        print("âœ“ é é¢åŒ…å«ã€Œå¾€ç¸¾ã€æ–‡æœ¬")
    else:
        print("âœ— é é¢ä¸åŒ…å«ã€Œå¾€ç¸¾ã€æ–‡æœ¬")
    
    # æŸ¥æ‰¾ div å®¹å™¨
    divs_with_history = soup.find_all('div', string=lambda x: x and 'å¾€ç¸¾' in x)
    print(f"âœ“ åŒ…å«ã€Œå¾€ç¸¾ã€æ–‡æœ¬çš„ div: {len(divs_with_history)}")
    
    # æŸ¥æ‰¾æ‰€æœ‰å«æœ‰æ•¸å­—å’Œæ—¥æœŸæ¨¡å¼çš„ table
    print("\nğŸ“ˆ å¯èƒ½çš„å¾€ç¸¾è¡¨æ ¼ (æ ¹æ“šå…§å®¹æ¨æ–·):")
    for idx, table in enumerate(tables):
        rows = table.find_all('tr')
        if len(rows) < 2:
            continue
        
        # æª¢æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æœ‰æ—¥æœŸæ ¼å¼
        first_row = rows[1]
        row_text = first_row.get_text()
        
        if any(pattern in row_text for pattern in ['/', '-', 'å¹´', 'æœˆ', 'æ—¥']):
            # å¯èƒ½åŒ…å«æ—¥æœŸ
            headers = table.find_all('th')
            print(f"  è¡¨ {idx}: {len(rows)} è¡Œ, {len(headers)} æ¬„ â† å¯èƒ½æ˜¯å¾€ç¸¾")
    
    # ============================================================
    # 4. åŒ¯å‡ºåŸå§‹ HTML (å‰ 3000 å­—ç¬¦)
    # ============================================================
    print("\n" + "=" * 70)
    print("4ï¸âƒ£ ç¬¬ä¸€å€‹è¡¨æ ¼çš„åŸå§‹ HTML")
    print("=" * 70 + "\n")
    
    if tables:
        first_table_html = str(tables[0])[:2000]
        print(first_table_html)
        print("\n[HTML å·²æˆªæ–·...]")
    
    # ============================================================
    # 5. å°å‡º JSON ä¾›åˆ†æ
    # ============================================================
    print("\n" + "=" * 70)
    print("5ï¸âƒ£ åŒ¯å‡ºè¨ºæ–·æ•¸æ“š")
    print("=" * 70 + "\n")
    
    diagnosis = {
        'horse_id': horse_id,
        'total_tables': len(tables),
        'tables': []
    }
    
    for idx, table in enumerate(tables):
        rows = table.find_all('tr')
        headers = table.find_all('th')
        
        table_info = {
            'index': idx,
            'class': table.get('class', []),
            'id': table.get('id', ''),
            'row_count': len(rows),
            'header_count': len(headers),
            'headers': [h.get_text(strip=True) for h in headers[:15]],
            'first_row': [cell.get_text(strip=True)[:30] for cell in rows[1].find_all('td')[:15]] if len(rows) > 1 else []
        }
        
        diagnosis['tables'].append(table_info)
    
    # ä¿å­˜ç‚º JSON
    json_str = json.dumps(diagnosis, ensure_ascii=False, indent=2)
    print("è¨ºæ–·æ•¸æ“šå·²å°å‡º (JSON):")
    print(json_str[:1000])
    print("\n[JSON å·²æˆªæ–·...]\n")
    
    return diagnosis


if __name__ == "__main__":
    # åŸ·è¡Œè¨ºæ–·
    diagnosis = diagnose_horse_page("HK_2023_J411")
    
    print("\n" + "=" * 70)
    print("ğŸ’¡ å»ºè­°")
    print("=" * 70)
    print("""
å¦‚æœå¾€ç¸¾è¡¨æ ¼ä»æœªè¢«æ‰¾åˆ°:

1. æª¢æŸ¥ä¸Šæ–¹è¼¸å‡ºï¼Œæ‰¾å‡ºæœ€å¯èƒ½çš„å¾€ç¸¾è¡¨æ ¼ç´¢å¼•
2. æ ¹æ“šè¡¨é ­å’Œå…§å®¹åˆ¤æ–·å“ªå€‹è¡¨æ ¼æ˜¯å¾€ç¸¾
3. æ›´æ–°çˆ¬èŸ²ä»£ç¢¼ï¼ŒæŒ‡å®šæ­£ç¢ºçš„è¡¨æ ¼æˆ–èª¿æ•´åŒ¹é…é‚è¼¯

å¸¸è¦‹å•é¡Œ:
- å¾€ç¸¾è¡¨æ ¼å¯èƒ½æ²’æœ‰ <th> è¡¨é ­ (åªæœ‰ <td>)
- å¾€ç¸¾å¯èƒ½åœ¨ç‰¹å®šçš„ div > table çµæ§‹ä¸­
- è¡¨æ ¼å¯èƒ½æœ‰å¤šå€‹å±¤ç´šæˆ–åµŒå¥—
- æ—¥æœŸæ ¼å¼å¯èƒ½èˆ‡é æœŸä¸åŒ

ä¸‹ä¸€æ­¥: è«‹å°‡è¨ºæ–·è¼¸å‡ºå…±äº«ï¼Œæˆ‘å€‘æœƒåŸºæ–¼å¯¦éš› HTML çµæ§‹èª¿æ•´çˆ¬èŸ²ã€‚
    """)
