# -*- coding: utf-8 -*-

"""
HKJC æ’ä½è¡¨è©³ç´°è³½æ¬¡ä¿¡æ¯æå–å™¨ v2.0 (HTML çµæ§‹åˆ†æç‰ˆ)
Race Details Extractor v2.0 - HTML Structure Analysis

æ”¹é€²:
1. ç›´æ¥è§£æå®Œæ•´ HTML
2. æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å®¹å™¨ (div, section, span ç­‰)
3. æå–å±¬æ€§ä¿¡æ¯ (class, id, data-* ç­‰)
4. è©³ç´°æ—¥èªŒè¼¸å‡º HTML çµæ§‹
5. å¤šç­–ç•¥æå–
"""

import re
import logging
from typing import Dict, List, Optional
import requests
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


class RaceDetailsExtractorV2:
    """è³½æ¬¡è©³ç´°ä¿¡æ¯æå– v2.0 - HTML çµæ§‹åˆ†æ"""
    
    BASE_URL = "https://racing.hkjc.com/zh-hk/local/information/racecard"
    
    def __init__(self, timeout=15):
        self.timeout = timeout
        self.session = requests.Session()
    
    def extract_race_details(self, race_date: str, racecourse: str, race_no: int) -> Dict:
        """å®Œæ•´çš„è³½æ¬¡è©³ç´°ä¿¡æ¯æå–"""
        
        try:
            url = f"{self.BASE_URL}?racedate={race_date}&Racecourse={racecourse}&RaceNo={race_no}"
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ” æå–è³½æ¬¡è©³ç´°ä¿¡æ¯")
            logger.info(f"URL: {url}")
            logger.info(f"{'='*80}\n")
            
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # 1. åˆ†æé é¢çµæ§‹
            logger.info("\nğŸ“Š é é¢çµæ§‹åˆ†æ...")
            self._analyze_page_structure(soup)
            
            # 2. å˜—è©¦å¤šç¨®æå–ç­–ç•¥
            logger.info("\nğŸ¯ å˜—è©¦æå–ç­–ç•¥...")
            race_details = self._extract_via_multiple_strategies(soup)
            
            return {
                'status': 'success',
                'url': url,
                'race_details': race_details
            }
        
        except Exception as e:
            logger.error(f"âŒ æå–å¤±æ•—: {str(e)}", exc_info=True)
            return {
                'status': 'error',
                'error': str(e)
            }
    
    def _analyze_page_structure(self, soup: BeautifulSoup):
        """åˆ†æé é¢çµæ§‹ï¼Œæ‰“å°æ‰€æœ‰ä¸»è¦å®¹å™¨"""
        
        # æ‰¾å‡ºæ‰€æœ‰ä¸»è¦å®¹å™¨
        logger.info("ğŸ“Œ ä¸»è¦å®¹å™¨æƒæ:")
        
        # æŸ¥æ‰¾ heading
        for tag_name in ['h1', 'h2', 'h3', 'h4', 'strong']:
            elements = soup.find_all(tag_name)
            if elements:
                logger.info(f"\n  <{tag_name}> ({len(elements)} å€‹):")
                for elem in elements[:3]:  # æœ€å¤šé¡¯ç¤º 3 å€‹
                    text = elem.get_text(strip=True)[:60]
                    logger.info(f"    - {text}")
        
        # æŸ¥æ‰¾ divs with id or class
        logger.info(f"\n  <div> with id/class ({len(soup.find_all('div'))} å€‹ div):")
        for div in soup.find_all('div')[:20]:  # æƒæå‰ 20 å€‹
            div_id = div.get('id', '')
            div_class = div.get('class', [])
            div_data = {k: v for k, v in div.attrs.items() if k.startswith('data-')}
            
            if div_id or div_class or div_data:
                text_preview = div.get_text(strip=True)[:40]
                logger.debug(f"    - id='{div_id}', class={div_class}, text='{text_preview}'")
        
        # æŸ¥æ‰¾ spans
        logger.info(f"\n  <span> ({len(soup.find_all('span'))} å€‹):")
        for span in soup.find_all('span')[:10]:
            span_class = span.get('class', [])
            text = span.get_text(strip=True)[:50]
            if span_class or len(text) > 5:
                logger.debug(f"    - class={span_class}, text='{text}'")
    
    def _extract_via_multiple_strategies(self, soup: BeautifulSoup) -> Dict:
        """ä½¿ç”¨å¤šç¨®ç­–ç•¥æå–è³½æ¬¡ä¿¡æ¯"""
        
        details = {}
        all_text = soup.get_text(separator=' ')
        
        logger.info("\nğŸ¯ æå–ç­–ç•¥ 1: æ­£å‰‡è¡¨é”å¼")
        self._strategy_regex(all_text, details)
        
        logger.info("\nğŸ¯ æå–ç­–ç•¥ 2: HTML æ¨™ç±¤æƒæ")
        self._strategy_html_scanning(soup, details)
        
        logger.info("\nğŸ¯ æå–ç­–ç•¥ 3: å®¹å™¨åˆ†æ")
        self._strategy_container_analysis(soup, details)
        
        return details
    
    def _strategy_regex(self, text: str, details: Dict):
        """ç­–ç•¥ 1: ä½¿ç”¨æ­£å‰‡è¡¨é”å¼å¾æ–‡æœ¬ä¸­æå–"""
        
        # è³½æ¬¡æ¨™é¡Œ (ç¬¬ 1 å ´ - ç¾åˆ©è®“è³½)
        match = re.search(r'ç¬¬\s*(\d+)\s*å ´\s*[ï¼-]?\s*(.+?)(?=\d{4}å¹´|\n|$)', text)
        if match:
            details['race_number'] = match.group(1).strip()
            details['race_name'] = match.group(2).strip()
            logger.info(f"  âœ“ è³½æ¬¡: {match.group(0)}")
        
        # æ—¥æœŸ (2026å¹´1æœˆ7æ—¥)
        match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
        if match:
            details['date'] = f"{match.group(1)}å¹´{match.group(2)}æœˆ{match.group(3)}æ—¥"
            logger.info(f"  âœ“ æ—¥æœŸ: {details['date']}")
        
        # æ˜ŸæœŸå¹¾
        match = re.search(r'(æ˜ŸæœŸ[ä¸€äºŒä¸‰å››äº”å…­æ—¥])', text)
        if match:
            details['day_of_week'] = match.group(1)
            logger.info(f"  âœ“ æ˜ŸæœŸ: {details['day_of_week']}")
        
        # é¦¬å ´ (è·‘é¦¬åœ°|æ²™ç”°)
        if 'è·‘é¦¬åœ°' in text:
            details['venue'] = 'è·‘é¦¬åœ°'
        elif 'æ²™ç”°' in text:
            details['venue'] = 'æ²™ç”°'
        if 'venue' in details:
            logger.info(f"  âœ“ é¦¬å ´: {details['venue']}")
        
        # æ™‚é–“ (18:40)
        match = re.search(r'(\d{2}):(\d{2})', text)
        if match:
            details['time'] = f"{match.group(1)}:{match.group(2)}"
            logger.info(f"  âœ“ æ™‚é–“: {details['time']}")
        
        # è³½é“é¡å‹ (è‰åœ°|å…¨å¤©å€™)
        if re.search(r'è‰åœ°(?!è³½)', text):
            details['track_type'] = 'è‰åœ°'
            logger.info(f"  âœ“ è³½é“é¡å‹: è‰åœ°")
        elif 'å…¨å¤©å€™' in text:
            details['track_type'] = 'å…¨å¤©å€™'
            logger.info(f"  âœ“ è³½é“é¡å‹: å…¨å¤©å€™")
        
        # è³½é“ç­‰ç´š ("A", "B", "C+3" ç­‰)
        match = re.search(r'["\"]([A-Z]+(?:\+\d+)?)["\"]', text)
        if match:
            details['track_rating'] = match.group(1)
            logger.info(f"  âœ“ è³½é“ç­‰ç´š: {details['track_rating']}")
        
        # è·é›¢ (1800ç±³)
        match = re.search(r'(\d{4})ç±³', text)
        if match:
            details['distance'] = match.group(1)
            logger.info(f"  âœ“ è·é›¢: {details['distance']}ç±³")
        
        # å ´åœ°ç‹€æ³ (å¥½åœ°|å¥½/å¿« ç­‰)
        match = re.search(r'(å¥½åœ°|å¥½/å¿«|å¿«åœ°|æ¿¡åœ°|çˆ›åœ°)', text)
        if match:
            details['going'] = match.group(1)
            logger.info(f"  âœ“ å ´åœ°: {details['going']}")
        
        # çé‡‘ ($875,000)
        match = re.search(r'\$\s*([\d,]+)', text)
        if match:
            details['prize_money'] = match.group(1).replace(',', '')
            logger.info(f"  âœ“ çé‡‘: ${details['prize_money']}")
        
        # è©•åˆ†ç¯„åœ (40-0)
        match = re.search(r'è©•åˆ†:\s*(\d+)[ï½-](\d+)', text)
        if match:
            details['rating_range'] = f"{match.group(1)}-{match.group(2)}"
            logger.info(f"  âœ“ è©•åˆ†: {details['rating_range']}")
        
        # ç­æ¬¡ (ç¬¬äº”ç­)
        match = re.search(r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­]ç­|\dç­)', text)
        if match:
            details['class'] = match.group(1)
            logger.info(f"  âœ“ ç­æ¬¡: {details['class']}")
    
    def _strategy_html_scanning(self, soup: BeautifulSoup, details: Dict):
        """ç­–ç•¥ 2: æƒæ HTML æ¨™ç±¤ä¸­çš„å…§å®¹"""
        
        # æƒææ‰€æœ‰ heading æ¨™ç±¤
        for heading in soup.find_all(['h1', 'h2', 'h3', 'strong']):
            text = heading.get_text(strip=True)
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«è³½æ¬¡ä¿¡æ¯
            if re.search(r'ç¬¬\s*\d+\s*å ´', text):
                logger.info(f"  âœ“ åœ¨ <{heading.name}> ä¸­æ‰¾åˆ°è³½æ¬¡: {text}")
                match = re.search(r'ç¬¬\s*(\d+)\s*å ´\s*[ï¼-]?\s*(.+)', text)
                if match and 'race_number' not in details:
                    details['race_number'] = match.group(1).strip()
                    details['race_name'] = match.group(2).strip()
    
    def _strategy_container_analysis(self, soup: BeautifulSoup, details: Dict):
        """ç­–ç•¥ 3: åˆ†æç‰¹å®šå®¹å™¨ (å¦‚ class='raceInfo', 'raceDetails' ç­‰)"""
        
        # å°‹æ‰¾å¯èƒ½åŒ…å«è³½æ¬¡ä¿¡æ¯çš„å®¹å™¨
        containers_keywords = ['race', 'info', 'detail', 'condition', 'track']
        
        for container in soup.find_all(['div', 'section']):
            container_class = ' '.join(container.get('class', []))
            container_id = container.get('id', '')
            
            # æª¢æŸ¥æ˜¯å¦åŒ¹é…é—œéµè©
            if any(kw in container_class.lower() or kw in container_id.lower() for kw in containers_keywords):
                text = container.get_text(strip=True)
                if len(text) > 20:  # é¿å…ç©ºå®¹å™¨
                    logger.debug(f"  - æ‰¾åˆ°ç›¸é—œå®¹å™¨: id='{container_id}', class='{container_class}'")
                    logger.debug(f"    å…§å®¹: {text[:100]}")


if __name__ == "__main__":
    extractor = RaceDetailsExtractorV2(timeout=15)
    
    result = extractor.extract_race_details("2026/01/07", "HV", 1)
    
    print("\n" + "="*80)
    print("ğŸ“‹ æå–çµæœ")
    print("="*80)
    
    if result['status'] == 'success':
        print("\nâœ… æå–æˆåŠŸï¼\n")
        for key, value in result['race_details'].items():
            print(f"  {key:20s}: {value}")
    else:
        print(f"\nâŒ æå–å¤±æ•—: {result['error']}")
